{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "Tensorflow.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmYw-7WvD2LG",
        "colab_type": "text"
      },
      "source": [
        "# Putting it all together\n",
        "\n",
        "Agenda:\n",
        "\n",
        "We will learn how to do the following\n",
        "\n",
        "1.   Importing data\n",
        "2.   Seting the hyperparameters\n",
        "3.   Creating the network\n",
        "4.   Creating the network graph\n",
        "5.   Training \n",
        "6.   Testing\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "<img src=\"nn.png\">\n",
        "\n",
        "___Fig. 1-___ Sample Neural Network architecture with two layers implemented for classifying MNIST digits\n",
        "\n",
        "\n",
        "Before doing any work, let's think based on what we spoke so far:\n",
        "\n",
        "**How many observations do you need**\n",
        "\n",
        "**Tips**\n",
        "* Hand-designed features - Neural Network\n",
        "* Image - Convolutional Neural Network\n",
        "* Sequential data (e.g. time) - Recurrent Neural Network\n",
        "* Decision making - Reinforcement Learning\n",
        "\n",
        "## Import the required libraries:\n",
        "We will start with importing the required Python libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLxfmfttD2LL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# imports\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1hAbkjFD2LQ",
        "colab_type": "text"
      },
      "source": [
        "## Importing MNIST data\n",
        "\n",
        "For this part of the workshop we use the MNIST dataset. MNIST is a dataset of handwritten digits. If you are into machine learning, you might have heard of this dataset by now. MNIST is kind of benchmark of datasets for deep learning and is easily accesible through Tensorflow\n",
        "\n",
        "The dataset contains $55,000$ examples for training, $5,000$ examples for validation and $10,000$ examples for testing. The digits have been size-normalized and centered in a fixed-size image ($28\\times28$ pixels) with values from $0$ to $1$. For simplicity, each image has been flattened and converted to a 1-D numpy array of $784$ features ($28\\times28$).\n",
        "\n",
        "<img src=\"files/files/mnist.png\">\n",
        "\n",
        "\n",
        "If you want to know more about the MNIST dataset you can check __Yann Lecun__'s [website](http://yann.lecun.com/exdb/mnist/).\n",
        "\n",
        "### Data dimension\n",
        "Here, we specify the dimensions of the images which will be used in several places in the code below. Defining these variables makes it easier (compared with using hard-coded number all throughout the code) to modify them later. Ideally these would be inferred from the data that has been read, but here we will just write the numbers.\n",
        "\n",
        "It's important to note that in a linear model, we have to flatten the input images into a vector. Here, each of the $28\\times28$ images are flattened into a $1\\times784$ vector. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYNCPEBPD2LS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_h = img_w = 28             # MNIST images are 28x28\n",
        "img_size_flat = img_h * img_w  # 28x28=784, the total number of pixels\n",
        "n_classes = 10                 # Number of classes, one class per digit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZ9-MF_bD2LW",
        "colab_type": "text"
      },
      "source": [
        "### Helper functions to load the MNIST data\n",
        "\n",
        "Here, we'll write the function which automatically loads the MNIST data and returns it in our desired shape and format. \n",
        "\n",
        "Here, we'll simply write a function (__`load_data`__) which has two modes: train (which loads the training and validation images and their corresponding labels) and test (which loads the test images and their corresponding labels). You can replace this function to use your own dataset. \n",
        "\n",
        "Other than a function for loading the images and corresponding labels, we define two more functions:\n",
        "\n",
        "1. __randomize__: which randomizes the order of images and their labels. This is important to make sure that the input images are sorted in a completely random order. Moreover, at the beginning of each __epoch__, we will re-randomize the order of data samples to make sure that the trained model is not sensitive to the order of data.\n",
        "\n",
        "2. __get_next_batch__: which only selects a few number of images determined by the batch_size variable (if you don't know why, read about Stochastic Gradient Method)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLBAsKtMD2LX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(mode='train'):\n",
        "    \"\"\"\n",
        "    Function to (download and) load the MNIST data\n",
        "    :param mode: train or test\n",
        "    :return: images and the corresponding labels\n",
        "    \"\"\"\n",
        "    from tensorflow.examples.tutorials.mnist import input_data\n",
        "    mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
        "    if mode == 'train':\n",
        "        x_train, y_train, x_valid, y_valid = mnist.train.images, mnist.train.labels, \\\n",
        "                                             mnist.validation.images, mnist.validation.labels\n",
        "        return x_train, y_train, x_valid, y_valid\n",
        "    elif mode == 'test':\n",
        "        x_test, y_test = mnist.test.images, mnist.test.labels\n",
        "    return x_test, y_test\n",
        "\n",
        "\n",
        "def randomize(x, y):\n",
        "    \"\"\" Randomizes the order of data samples and their corresponding labels\"\"\"\n",
        "    permutation = np.random.permutation(y.shape[0])\n",
        "    shuffled_x = x[permutation, :]\n",
        "    shuffled_y = y[permutation]\n",
        "    return shuffled_x, shuffled_y\n",
        "\n",
        "\n",
        "def get_next_batch(x, y, start, end):\n",
        "    x_batch = x[start:end]\n",
        "    y_batch = y[start:end]\n",
        "    return x_batch, y_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuoWYcKiD2Lb",
        "colab_type": "text"
      },
      "source": [
        "### 1.3. Load the data and display the sizes\n",
        "Now we can use the defined helper function in __train__ mode which loads the train and validation images and their corresponding labels. We'll also display their sizes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SBLjgDDD2Ld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load MNIST data\n",
        "x_train, y_train, x_valid, y_valid = load_data(mode='train')\n",
        "print(\"Size of:\")\n",
        "print(\"- Training-set:\\t\\t{}\".format(len(y_train)))\n",
        "print(\"- Validation-set:\\t{}\".format(len(y_valid)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqsx0WElD2Lk",
        "colab_type": "text"
      },
      "source": [
        "To get a better sense of the data, let's checkout the shapes of the loaded arrays."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3qCY3CcD2Ll",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('x_train:\\t{}'.format(x_train.shape))\n",
        "print('y_train:\\t{}'.format(y_train.shape))\n",
        "print('x_train:\\t{}'.format(x_valid.shape))\n",
        "print('y_valid:\\t{}'.format(y_valid.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mp1ZGYhZD2Lq",
        "colab_type": "text"
      },
      "source": [
        "As you can see, __`x_train`__ and __`x_valid`__ arrays contain $55000$ and $5000$ flattened images ( of size  $28\\times28=784$  values). __`y_train`__ and __`y_valid`__ contain the corresponding labels of the images in the training and validation set respectively. \n",
        "\n",
        "Based on the dimesnion of the arrays, for each image, we have 10 values as its label. Why? This technique is called __One-Hot Encoding__. This means the labels have been converted from a single number to a vector whose length equals the number of possible classes. All elements of the vector are zero except for the $i^{th}$ element which is one and means the class is $i$. For example, the One-Hot encoded labels for the first 5 images in the validation set are:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "A0r-cWJpD2Lr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_valid[:5, :]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvVSvMpdD2Lv",
        "colab_type": "text"
      },
      "source": [
        "where the $10$ values in each row represents the label assigned to that partiular image. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpfV2KRBD2Lx",
        "colab_type": "text"
      },
      "source": [
        "## Hyperparameters\n",
        "\n",
        "Here, we have about $55,000$ images in our training set. It takes a long time to calculate the gradient of the model using all these images. We therefore use __Stochastic Gradient Descent__ which only uses a small batch of images in each iteration of the optimizer. Let's define some of the terms usually used in this context:\n",
        "\n",
        "- __epoch__: one forward pass and one backward pass of __all__ the training examples.\n",
        "- __batch size__: the number of training examples in one forward/backward pass. The higher the batch size, the more memory space you'll need.\n",
        "- __iteration__: one forward pass and one backward pass of __one batch of images__ the training examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyT0zD6eD2Lz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyper-parameters\n",
        "epochs = 10             # Total number of training epochs\n",
        "batch_size = 100        # Training batch size\n",
        "display_freq = 100      # Frequency of displaying the training results\n",
        "learning_rate = 0.001   # The optimization initial learning rate\n",
        "\n",
        "h1 = 200                # number of nodes in the 1st hidden layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KL1S4Bv0D2L3",
        "colab_type": "text"
      },
      "source": [
        "Given the above definitions, each epoch consists of $55,000/100=550$ iterations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hn42_I1DD2L4",
        "colab_type": "text"
      },
      "source": [
        "## Creating the network\n",
        "\n",
        "### Helper functions for creating new variables\n",
        "\n",
        "As explained (and also illustrated in Fig. 1), we need to define two variables $\\mathbf{W}$ and $\\mathbf{b}$ to construt our linear model. These are generally called model parameters and we use __Tensorflow Variables__ of proper size and initialization to define them.The following functions are written to be later used for generating the weight and bias variables of the desired shape:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ntvUZEJD2L6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# weight and bais wrappers\n",
        "def weight_variable(name, shape):\n",
        "    \"\"\"\n",
        "    Create a weight variable with appropriate initialization\n",
        "    :param name: weight name\n",
        "    :param shape: weight shape\n",
        "    :return: initialized weight variable\n",
        "    \"\"\"\n",
        "    initer = tf.truncated_normal_initializer(stddev=0.01)\n",
        "    return tf.get_variable('W_' + name,\n",
        "                           dtype=tf.float32,\n",
        "                           shape=shape,\n",
        "                           initializer=initer)\n",
        "\n",
        "\n",
        "def bias_variable(name, shape):\n",
        "    \"\"\"\n",
        "    Create a bias variable with appropriate initialization\n",
        "    :param name: bias variable name\n",
        "    :param shape: bias variable shape\n",
        "    :return: initialized bias variable\n",
        "    \"\"\"\n",
        "    initial = tf.constant(0., shape=shape, dtype=tf.float32)\n",
        "    return tf.get_variable('b_' + name,\n",
        "                           dtype=tf.float32,\n",
        "                           initializer=initial)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLkBfDfXD2L_",
        "colab_type": "text"
      },
      "source": [
        "### Helper-function for creating a fully-connected layer\n",
        "\n",
        "Neural network consists of stacks of fully-connected (dense) layers. Having the weight ($\\mathbf{W}$) and bias ($\\mathbf{b}$) variables, a fully-connected layer is defined as $activation(\\mathbf{W}\\times \\mathbf{x} + \\mathbf{b})$. We define __`fc_layer`__ function as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XRqAf6LD2MA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fc_layer(x, num_units, name, use_relu=True):\n",
        "    \"\"\"\n",
        "    Create a fully-connected layer\n",
        "    :param x: input from previous layer\n",
        "    :param num_units: number of hidden units in the fully-connected layer\n",
        "    :param name: layer name\n",
        "    :param use_relu: boolean to add ReLU non-linearity (or not)\n",
        "    :return: The output array\n",
        "    \"\"\"\n",
        "    in_dim = x.get_shape()[1]\n",
        "    W = weight_variable(name, shape=[in_dim, num_units])\n",
        "    b = bias_variable(name, [num_units])\n",
        "    layer = tf.matmul(x, W)\n",
        "    layer += b\n",
        "    if use_relu:\n",
        "        layer = tf.nn.relu(layer)\n",
        "    return layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heNL_hdZD2ME",
        "colab_type": "text"
      },
      "source": [
        "## Create the network graph\n",
        "\n",
        "Now that we have defined all the helped functions to create our model, we can create our network.\n",
        "\n",
        "### Placeholders for the inputs (x) and corresponding labels (y)\n",
        "\n",
        "First we need to define the proper tensors to feed in the input values to our model. Placeholder variable is the suitable choice for the input images and corresponding labels. This allows us to change the inputs (images and labels) to the TensorFlow graph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWjbxaxOD2MG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the graph for the linear model\n",
        "# Placeholders for inputs (x) and outputs(y)\n",
        "x = tf.placeholder(tf.float32, shape=[None, img_size_flat], name='X')\n",
        "y = tf.placeholder(tf.float32, shape=[None, n_classes], name='Y')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPOi-mA4D2MM",
        "colab_type": "text"
      },
      "source": [
        "Placeholder __`x`__ is defined for the images; its data-type is set to __`float32`__ and the shape is set to __[None, img_size_flat]__, where __`None`__ means that the tensor may hold an arbitrary number of images with each image being a vector of length __`img_size_flat`__.\n",
        "\n",
        "\n",
        "Next we have __`y`__ which is the placeholder variable for the true labels associated with the images that were input in the placeholder variable __`x`__. The shape of this placeholder variable is __[None, num_classes]__ which means it may hold an arbitrary number of labels and each label is a vector of length __`num_classes`__ which is $10$ in this case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9gJHdBdD2MN",
        "colab_type": "text"
      },
      "source": [
        "### Create the network layers\n",
        "\n",
        "\n",
        "After creating the proper input, we have to pass it to our model. Since we have a neural network, we can stack multiple fully-connected layers using __`fc_layer`__ method. Note that we will not use any activation function (`use_relu=False`) in the last layer. The reason is that we can use `tf.nn.softmax_cross_entropy_with_logits` to calculate the `loss`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8GkwVNuD2MP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a fully-connected layer with h1 nodes as hidden layer\n",
        "fc1 = fc_layer(x, h1, 'FC1', use_relu=True)\n",
        "# Create a fully-connected layer with n_classes nodes as output layer\n",
        "output_logits = fc_layer(fc1, n_classes, 'OUT', use_relu=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjzEc4AgD2MT",
        "colab_type": "text"
      },
      "source": [
        "### Define the loss function, optimizer, accuracy, and predicted class\n",
        "\n",
        "After creating the network, we have to calculate the loss and optimize it. Also, to evaluate our model, we have to calculate the `correct_prediction` and `accuracy`. We will also define `cls_prediction` to visualize our results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCatSExJD2MU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the loss function, optimizer, and accuracy\n",
        "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=output_logits), name='loss')\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, name='Adam-op').minimize(loss)\n",
        "correct_prediction = tf.equal(tf.argmax(output_logits, 1), tf.argmax(y, 1), name='correct_pred')\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy')\n",
        "\n",
        "# Network predictions\n",
        "cls_prediction = tf.argmax(output_logits, axis=1, name='predictions')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FplpWriD2MZ",
        "colab_type": "text"
      },
      "source": [
        "### Initialize all variables\n",
        "\n",
        "Here, we have to invoke a variable initializer operation to initialize all variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhseSKJTD2Ma",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the op for initializing all variables\n",
        "init = tf.global_variables_initializer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQnZf90tD2Mk",
        "colab_type": "text"
      },
      "source": [
        "## Train\n",
        "\n",
        "After creating the graph, it is time to train our model. To train the model, we have to create a session and run the graph in our session."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "n3TfhOYRD2Mn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create an interactive session (to keep the session in the other cells)\n",
        "sess = tf.InteractiveSession()\n",
        "# Initialize all variables\n",
        "sess.run(init)\n",
        "# Number of training iterations in each epoch\n",
        "num_tr_iter = int(len(y_train) / batch_size)\n",
        "for epoch in range(epochs):\n",
        "    print('Training epoch: {}'.format(epoch + 1))\n",
        "    # Randomly shuffle the training data at the beginning of each epoch \n",
        "    x_train, y_train = randomize(x_train, y_train)\n",
        "    for iteration in range(num_tr_iter):\n",
        "        start = iteration * batch_size\n",
        "        end = (iteration + 1) * batch_size\n",
        "        x_batch, y_batch = get_next_batch(x_train, y_train, start, end)\n",
        "\n",
        "        # Run optimization op (backprop)\n",
        "        feed_dict_batch = {x: x_batch, y: y_batch}\n",
        "        sess.run(optimizer, feed_dict=feed_dict_batch)\n",
        "\n",
        "        if iteration % display_freq == 0:\n",
        "            # Calculate and display the batch loss and accuracy\n",
        "            loss_batch, acc_batch = sess.run([loss, accuracy],\n",
        "                                             feed_dict=feed_dict_batch)\n",
        "\n",
        "            print(\"iter {0:3d}:\\t Loss={1:.2f},\\tTraining Accuracy={2:.01%}\".\n",
        "                  format(iteration, loss_batch, acc_batch))\n",
        "\n",
        "    # Run validation after every epoch\n",
        "    feed_dict_valid = {x: x_valid[:1000], y: y_valid[:1000]}\n",
        "    loss_valid, acc_valid = sess.run([loss, accuracy], feed_dict=feed_dict_valid)\n",
        "    print('---------------------------------------------------------')\n",
        "    print(\"Epoch: {0}, validation loss: {1:.2f}, validation accuracy: {2:.01%}\".\n",
        "          format(epoch + 1, loss_valid, acc_valid))\n",
        "    print('---------------------------------------------------------')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8Zi_QhJD2M2",
        "colab_type": "text"
      },
      "source": [
        "## Test\n",
        "\n",
        "After the training is done, we have to test our model to see how good it performs on a new dataset. There are multiple approaches to for this purpose. We will use two different methods.\n",
        "\n",
        "## Accuracy\n",
        "One way that we can evaluate our model is reporting the accuracy on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tpJhlnJD2M3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test the network after training\n",
        "# Accuracy\n",
        "x_test, y_test = load_data(mode='test')\n",
        "feed_dict_test = {x: x_test[:1000], y: y_test[:1000]}\n",
        "loss_test, acc_test = sess.run([loss, accuracy], feed_dict=feed_dict_test)\n",
        "print('---------------------------------------------------------')\n",
        "print(\"Test loss: {0:.2f}, test accuracy: {1:.01%}\".format(loss_test, acc_test))\n",
        "print('---------------------------------------------------------')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEWrhBpdD2NC",
        "colab_type": "text"
      },
      "source": [
        "## plot some results\n",
        "Another way to evaluate the model is to visualize the input and the model results and compare them with the true label of the input. This is advantages in numerous ways. For example, even if you get a decent accuracy, when you plot the results, you might see all the samples have been classified in one class. Another example is when you plot, you can have a rough idea on which examples your model failed. Let's define the helper functions to plot some correct and missclassified examples.\n",
        "\n",
        "### Helper functions for plotting the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RafOARYTD2NE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_images(images, cls_true, cls_pred=None, title=None):\n",
        "    \"\"\"\n",
        "    Create figure with 3x3 sub-plots.\n",
        "    :param images: array of images to be plotted, (9, img_h*img_w)\n",
        "    :param cls_true: corresponding true labels (9,)\n",
        "    :param cls_pred: corresponding true labels (9,)\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(3, 3, figsize=(9, 9))\n",
        "    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
        "    for i, ax in enumerate(axes.flat):\n",
        "        # Plot image.\n",
        "        ax.imshow(images[i].reshape(28, 28), cmap='binary')\n",
        "\n",
        "        # Show true and predicted classes.\n",
        "        if cls_pred is None:\n",
        "            ax_title = \"True: {0}\".format(cls_true[i])\n",
        "        else:\n",
        "            ax_title = \"True: {0}, Pred: {1}\".format(cls_true[i], cls_pred[i])\n",
        "\n",
        "        ax.set_title(ax_title)\n",
        "\n",
        "        # Remove ticks from the plot.\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "\n",
        "    if title:\n",
        "        plt.suptitle(title, size=20)\n",
        "    plt.show(block=False)\n",
        "\n",
        "\n",
        "def plot_example_errors(images, cls_true, cls_pred, title=None):\n",
        "    \"\"\"\n",
        "    Function for plotting examples of images that have been mis-classified\n",
        "    :param images: array of all images, (#imgs, img_h*img_w)\n",
        "    :param cls_true: corresponding true labels, (#imgs,)\n",
        "    :param cls_pred: corresponding predicted labels, (#imgs,)\n",
        "    \"\"\"\n",
        "    # Negate the boolean array.\n",
        "    incorrect = np.logical_not(np.equal(cls_pred, cls_true))\n",
        "\n",
        "    # Get the images from the test-set that have been\n",
        "    # incorrectly classified.\n",
        "    incorrect_images = images[incorrect]\n",
        "\n",
        "    # Get the true and predicted classes for those images.\n",
        "    cls_pred = cls_pred[incorrect]\n",
        "    cls_true = cls_true[incorrect]\n",
        "\n",
        "    # Plot the first 9 images.\n",
        "    plot_images(images=incorrect_images[0:9],\n",
        "                cls_true=cls_true[0:9],\n",
        "                cls_pred=cls_pred[0:9],\n",
        "                title=title)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEa1rgY5D2NI",
        "colab_type": "text"
      },
      "source": [
        "### Visualize correct and missclassified examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJF9rt3nD2NJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot some of the correct and misclassified examples\n",
        "cls_pred = sess.run(cls_prediction, feed_dict=feed_dict_test)\n",
        "cls_true = np.argmax(y_test[:1000], axis=1)\n",
        "plot_images(x_test, cls_true, cls_pred, title='Correct Examples')\n",
        "plot_example_errors(x_test[:1000], cls_true, cls_pred, title='Misclassified Examples')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OeALfy5D2NQ",
        "colab_type": "text"
      },
      "source": [
        "After we finished, we have to close the __`session`__ to free the memory. We could have also used:\n",
        "```python\n",
        "with tf.Session as sess:\n",
        "    ...\n",
        "```\n",
        "\n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIf2YoruD2NT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess.close()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}